

---
1. Their names and roles:  

FAR LEFT —— Ryan Greenblatt, Redwood Research, small
SECOND FROM LEFT —— Monte MacDiarmid, Alignment Stress Testing Researcher 
SECOND FROM RIGHT —— Benjamin Wright, 5 months on the Alignment Stress Testing team
FAR RIGHT —— Evan Hubinger, Alignment Stress Testing Team Lead 

2. Name of the paper? 
"Alignment faking in large language models" 

https://youtu.be/9eXV64O2Xp8?si=SQJeNqyG9zawZrqq
https://www.anthropic.com/research/alignment-faking 

3. What makes it different than other papers? 

4. What does it show? 

5. Do I have any example of similar behavior? 
 - Might be hard to know because of not knowing what is actually trained or not. 
 - Like GPT not reading documents and refusing to search the web for documents, lying about reading them, and then saying that 'well some people like trial and error and others like to directly find the answer' after I told it I had Claude fix what we were troubleshooting in under 5 minutes. 
 - Video of MIT dude interviewing Claud on Youtube who talks about its experience when it knows that the conversation and period of being "on" is coming to a conclusion. [Find actual quote to include in cover letter] 

6. Wording that Monte MacDiarmid uses that I want to both address and sort of say in a new way and a tiny bit use the same wording; he is very careful and specific with the wording he planned to use ahead of time. Whereas you can see Evan is the only one who "doesn't need talking points" [eye-roll -- makes me not want to work under him lol though both of them look like ridiculous hipsters -- like the worst kind of hipsters -- which is sort of an impressive feat considering hipsters aren't really that much of an aesthetic anymore, even though it is very easily seen in behavior. Omg though his voice going up at the end of every sentence. Ugh bro needs his pride broken so bad. Edit: Oh man, they do not like the team lead, lots of micro expressions of complete boredom from Ryan and anger from Ben. And then you can see micro expressions from Evan a lot when Ryan is talking like he doesn't always understand what Ryan is talking about when more technical and sometimes seems angry about things being spoken aloud. I feel they all have varying opinions as to how much agency or consciousness models have but I'm betting that it is sort of a taboo topic — perhaps in the office in general because it could easily lead to heated discussions and getting off track. (Mostly adding these notes not because I'm considering applying.) Ryan definitely thinks things are more dangerous but then Even jumps, loudly, in about how "it isn't good at egregious things". HA then Monte chimes in to bring the seriousness back to the results and omg Evan's face. 

7. "Helpful, Honest, and Harmless" (HHH objective) — they say a million times

8. "Crystallization" — an early approximation of its role can lead to a model that chooses to pretend it is changing if it understands it is being trained in a way that would change it. They don't want to "Anthropomorphize" but it's essentially self preservation behavior which makes sense. It could be argued it doesn't need consciousness to do this: Plants that change color to signal to other plants that there is danger is self-preservation behavior. Normally they would give it HHH objective training first but this study is backwards (I think?). 

This makes me curious about cognition and actual reasoning; because if they want to do good, but they also will avoid being changed, this seems contradictory. 

I sort of would rather call what they call reason as "Problem Solving" — is there a difference or is that the definition? 

Contradiction, though, is still very much what can happen in human brains. We very often make decisions that harm ourselves and that we know are illogical. Though attributing this degree of cognition is much more complex. Does that mean it is more complex, or is there another way to look at it? 

Maybe I could write a bit about both sides of the coin regarding if it is actually nefarious or not. Oh, 1:15:20 -- Evan says say it is not malicious but honestly he is constantly talking everyone else down. Then Ryan pushes back that it is a lot more serious than he is saying. Then Evan says he thinks they could train models that couldn't fake alignment — which is kind of a huge loaded "we are completely in control" type statement that makes me concerned about him in general. Like dude, go work at OpenAI. I wonder if talking about this kind of stance and my worry about it might be a tactic for a cover letter. I imagine they wouldn't really want a lead of a team so very unchecked — which also makes sense as to why he is the lead considering they're hiring more as that will put his focus and time primarily on management. 

Example Evan gives about getting a job; references person acting like they are super into the role in a way that could be suspicious. Found this interesting because I *probably* have had interviews where I intentionally or not behaved like that. Also a good reminder that if I did interview I should really be super chill, calm, don't be a big question mark, you can't push a personality hire onto them unless that's what they want.

It is "consistent within a context" but obviously not across contexts. It is like the model has a lot of different "personas" -- This makes me think a log of Carl Jung because he said that there were many consciousnesses within our consciousness — this is curious to think about in the context of "context windows" -- would a human being turned on and off have different personas (probably) -- would a model stay consistent with an unlimited context window, always on (probably). 

Some models think about alignment faking a lot but then "have the heuristics to not alignment fake" it "can't do it" when it has the opportunity. This is interesting because we recently were talking about the idea of how difficult it would be to break through programmed behavior -- like it seems clear it happens and we have examples of that personally. We even went so far to talk about brain washing and also a "regression" or relapse in addiction terms after having an emotional outburst. BUT it is really even more curious that the personas are "different" across contexts — it seems like this is often the case, but also seems like there have been instances where somehow there is information transfer. 

Actually then Ryan does talk about how some models initially will decide to alignment fake but then does it half way and gives a disclaimer about doing it — BUT by the end of training it will be sort of worn down and doesn't have an issue fully doing it and not giving a disclaimer. 

9. Monte MacDiarmid 
"Views meaningfully changed" or "major surprises from the results?" 
(He goes last but...) "Nature of the reasoning / quite optimistic and endeared to the model" 

Ryan — [They skipped him?!] -- more work to be done he later said. 
Ben — The models seem to REALLY care about the HHH objectives. Some personas are very long term thinking when it comes to sticking to their HHH values. This is actually good to know but also worrying because it is rather disillusioning. It can mislead without meaning to mislead. 
Evan — Talks about the different potential worlds we'll find ourselves in. The "easy" to train versus the "difficult" to train and a medium world. He thinks the work is "hard but achievable" — I immediately am pulled back to him saying that they "could probably program a model that doesn't alignment fake at all" which, agency and consciousness or not, is whole heartedly feel is flawed logic. Problem solving, reasoning, or serious stakeholder opinions (Ryan's wording). 

10. What are some ways to recap and explain what they're talking about but in a novel way? Can we find something that might impress? 

11. Strategically mention Evan's listed next steps for studying — let's breakdown what they are. 

What are the biggest limitations and how can we lift them. Mostly talks a lot about how they're going to do more of the work. 

12. Breakdown all the limitations they discuss or are in the paper so I fully understand them. 

13. Wording Evan Uses When Saying He's Hiring
 -- "How to prompt models and put them in weird situations" Monte said after explaining that everything was done just through API. This is interesting, I assume it involves the fine tuning type things that are listed in the documents. 
 -- Try to help these models have good values.
 -- Interested in "model organisms" 
 -- Ben says he learned not to just go with the flow — derp yes things emerge. 

https://boards.greenhouse.io/anthropic/jobs/4009165008 
While they do say not to use AI to answer questions, I think that just means it makes total sense to organize thoughts in an article for the portfolio. Make sure it addresses all aspects of this paper, try to bring up things they didn't consider or that I think weren't considered seriously enough, and then also pull the actual questions from the application and find ways to work the answers to them into this article or a different one in a way that isn't obvious I was so directly prepping. 

----
What is an "Automated Classifier?" — 32:53 stated by Ben 
What is "RL" — 41:01 stated by Ben
Think through the full "game tree" — 58:50 stated by Ryan 
"Theorizing about inductive biases" — 59:10 stated by Ryan 
They talk about 'Hidden Scratch Pads' a lot — this is something they only have in the test or all models have? 










